* はじめに

　ある日、イチゴメロンパンを売っている会社で働くもふもふちゃんは、上司からいきなりこんなことを言われました。

　「もふもふちゃん、最近イチゴメロンパンの売れ行きが悪いんだ。ちょっと通販用のWebサイトにどのくらいの人がアクセスしているか
調べてくれないかな。日付ごとのグラフとかあると嬉しいなあ。」

　「あ、そういえばアクセス元もわかるといいよね。将来は海外展開もしたいしね！」

　イチゴメロンパンの通販サイトはWebサーバのApache上に構築されているため、Apacheログを解析すればユーザのアクセス数は調べることができそうです。

　「ええー、このログ全部調べるの…」
#@# Apacheログを入れる

　みてみたところ、Apacheのログはjson形式のようです。アクセス数を数えるためにはリクエストがGETのものだけ数える必要があります。
悪いことに、Webサーバとして利用されているマシンは他の機器とも通信しているようです。人がアクセスした履歴を追いかけるためには
使用している機器のIPアドレスを全て調べ、当てはまるログは解析対象から除外しなければなりません。なかなか骨が折れる作業です。

　…数日後、なんとか1週間分のアクセス数とアクセス元を出すことができました。ログの読みすぎで頭はくらくら、目はチカチカします。
そんなもふもふちゃんは解析結果を上司に提出しました。OKをもらえたので立ち去ろうとしたとき、今度はこんな頼まれごとをされてしまいました。

　「もふもふちゃん、そーいえば最近はTwitterの拡散力は大きいみたいだね。うちのイチゴメロンパンはどのくらいつぶやかれてるのか知りたいなあ」
#@# 挿絵入れる

　しかも、今度もつぶやきの発信元が知りたいと言われてしまいました。しかも、こんなことまでリクエストされてしまいます。
・情報の発信元は地図にプロットしてほしい
・「イチゴメロンパン」と入った単語のつぶやき数の推移を閲覧したい
・ここ3ヶ月分のつぶやき数がわかるとさらに良い

　「そんなー！Twitterの発信元とかどうやって調べたらいいの？だいたいイチゴメロンパンって単語が入ったつぶやき数なんて数えられないよ！
どーしたらいいのー？？」
　もふもふちゃんはすっかり困り果ててしまいました。
#@# 挿絵入れる

　…ここまで行かないにしろ、最近ではあるサービスの売り上げを伸ばすためであったり、自分の作った作品を宣伝するためには
インターネット上に溢れる情報や、自前のWebサイトへのアクセス履歴を分析する場面が多くなっています。理由としては
アクセスしてくるユーザーに対してダイレクトなマーケティング戦略を取るためであったり、人気の推移を研究することで
より買ってもらえる商品を生み出すためであったりと様々です。

　アクセス履歴はアクセスログから追いかけることがほとんどです。もふもふちゃんはログを手当たり次第に読んでいったので
すぐに疲れてしまいました。

　「もっと楽に解析できないのー？」
#@# 挿絵入れる

　…それができるんです。そう、Elastic Stackであれば！
　Elastic StackはオランダのElastic社が提供しているBIツールです。「BIツール」とは、企業や世の中にある色々な情報を集めて分析することを
支援するツールです。このElastic Stackですが、基本はOSSとして提供されており、コードはGithub上で管理されています。
日本でも少しずつ使われる場面が増えてきているようですが、まだまだ知見が少ないというのが現状です。

　ただ！このElastic Stackを使えば、もふもふちゃんが今依頼されていた仕事は全部簡単にできてしまいます。

・Webサイトへのアクセス履歴（Apacheログ)から特定のIP以外がアクセス元となっているものを抜き出したい
・情報の発信元は地図にプロットしたい
・「イチゴメロンパン」と入った単語のTwitter上でのつぶやき数の推移を閲覧したい（3ヶ月分）

　情報更新はほぼリアルタイムにすることが可能ですし、何よりいちいち生ログを閲覧する必要がありません。なんだかとても便利そうです。

　「ちょっとこのElastic Stackってやつ、自分の環境に入れてみようかな」

　みなさんも、もふもふちゃんと一緒に快適なログ解析を始めてみませんか？今までデバックするときくらいにしか使われていなかった
ゴミ同然のログを、ログ解析の力で宝の山に変えることができるかもしれません。

#@# 挿絵入れる

* おことわり
　この本で取り扱っている各ツールのバージョンはElasticSearch、Logstash、Kibana共に「5.2」を使用しています。
Elastic Stackはバージョンアップがかなり早いツールです。バージョンによって挙動がかなり違うため、別バージョンを使用した場合と
コンフィグの書き方や操作方法が異なる場合があります。あらかじめご了承ください。

　この本はログの分析方法をメインに扱う本です。そのため検索エンジンとしての
ElasticSearchのスキーマ設計など、性能チューニング系のトピックは取り上げません。

　筆者のPC環境はmacOS Serria （Ver：10.12.2）です。また、この本ではbrewを用いたElastic Stackのインストールは行いません。

　筆者のお財布事情により、本文はモノクロ印刷です。この本ではKibanaの画面キャプチャが複数回出てきますが
モノクロなことをお許しください。本当はカラフルな画面なので、公式サイトや画像検索などでKibana5を参照してみていただけると
テンションが上がると思います。

　この本の情報はElastic社の公式ドキュメント（URL：https://www.elastic.co/guide/index.html）を元に作成していますが、
本の情報を用いた開発・制作・運用に対して発生した全ての結果に対して責任は負いません。必ずご自身の環境でよく検証してから導入をお願いします。

* Elastic Stackって何？

　「Elastic StackはElastic社が提供しているBIツール群っていうのはわかったけど、どんなツールがあるのかな？
公式サイトを見るといっぱい種類があるみたいだけど…。」

#@# 挿絵入れる

　おや？もふもふちゃん、なんだかお困りのようです。それもそうですね。ELastic Stackには
なんだかたくさん便利ツールがあるのはわかりますが、ログ解析にはどのツールが必要なのかわかりません。
この章でログ分析に最低限必要なツールが何か、一緒にみてみましょう。

** Logstash
　Logstashは、各環境に散らばっているログを集め、指定した対象に連携できるツールです。ログの連携だけではなく、
ログの加工機能も持ち合わせています。コード自体はRuby言語で記載されています。

　肝心のどんなログが取り込みできるかですが、の出力形式としてよくあるテキストファイルはもちろん、xmlやjsonファイルも対象として指定できます。
ファイルの情報以外にも、Twitter APIと連携してTwitterのつぶやき情報を取り込む事や、
データベース（RSDB）に接続して情報を抜いてくる事も可能です。RSDBと連携する際はSQL文を用いて情報を取得するため
SQLが得意な人は自分の欲しい情報だけSQLで取得し、あとはログを連携するだけといった事も実現できます。

　ログの出力先ですが、このあと出てくるElasticSearchだけでなく、プロジェクトの進捗状況管理ツールである
Redmineに送付する事もできます。取り込みした情報をCSVファイルとして出力したりsyslogとして転送することができるので
利用方法によってはログ解析以上の威力を発揮するツールだと言えます。

** ElasticSearch
　ElasticSearchは、Javaで作られている分散処理型の検索エンジンです。クラスタ構成を組むことができるのが特徴なので、
大規模な環境で検索エンジンとして利用されることが多いです。某新聞のWebサイト内の検索、
Dockerのコンテナ検索、Facebook上での検索などが導入事例として有名です。

　クラスタとは、物理的には複数存在しているにも関わらず、論理的には1つとして見せることができる技術です。
処理の負荷分散ができるため、性能を求められる環境で選択されることが多いです。
#@# クラスターを解説した図を入れる

　もちろん、ログ解析を行うときもよく使用されます。理由としては次に出てくるKibanaの情報取得元が
ElasticSearchに限られているからです。

** Kibana
　Kibanaは、ElasticSearchに貯められている情報を整形して可視化する情報分析ツールです。
開発言語はアナウンスは出ていないものの、ソース情報を見る限りJavaScriptがメインだと思われます。
Google Chrome等のブラウザからKibana指定のURLにアクセスすることで、このようなグラフをすぐ表示することができます。

#@# Kibanaのキャプチャを入れる

　Kibanaでは知りたい情報の件数だけでなく、折れ線グラフ・棒グラフ・円グラフを用いてログの詳細な情報を解析し
色分けして表示することが可能です。グラフの大きさを決める際にコンフィグなどを編集する必要はなく、ブラウザ上での操作で
全て完結する仕組みとなっています。

** Beats
　Beatsはサーバにインストールすることで、サーバ内のマシンデータをElasticSearchやLogstashに転送する
簡易的なデータ収集ツールです。サーバのリソース情報以外にもネットワークのパケット情報・Windowsのイベントログを収集することが
できるため、Logstashでカバーしきれない範囲の情報を集めてくることができます。
Logstashにログを転送することで、他のログと同じように加工・転送が可能となるため、痒いところに手がとどくツールという位置づけです。

　BeatsはGo言語で作られており、開発初期から大幅に機能追加がされているツールです。今後どのように拡張されていくのか、楽しみですね。

** 基本的な構成

　今回はBeatsは扱わず、ログを収集するLogstash、ログを貯めておくElasticSearch、ログを閲覧するKibanaを基本構成とします。
この3つのツールで構成されている状態は「ELK」と省略されて呼ばれることが多いです。この本でもこれ以降はLogstash、ElasticSearch、Kibanaの
3つをまとめて扱う際は「ELK」と省略して呼ぶこととします。

#@# 簡単な構成図を差し入れる

* 環境構築（各ツールのインストール）

　「よーし、ELKがどんなものかだいたい理解できたから、インストールしてみよ！
…あれ、なんかインストール方法もいっぱいあるみたい。どれを選べばいいのかなあ？」

#@# 挿絵入れる

　あら？もふもふちゃん、今度はインストールで詰まってしまったようです。
公式サイトからダウンロードでzipファイルを落としてくることができるようですが、他のやり方もあるようです。
状況に合っている1番いいやり方を選択したいですよね。一緒にみてみましょう。

** インストールの順番
　インストールの前に、どのツールからインストールするかを決めておきましょう。ELKのデータの流れを考えると、
ElasticSearch→Logstash→Kibanaの順にインストールすることをお勧めします。
理由は図X.Xにあるように、Logstashで取りこんだデータをElasticSearchに連携するため
先にデータの連携先をセットアップしておかないと正しく動作確認ができない可能性があるためです。

#@# データの流れ図を入れる

　また、Kibanaは起動前にElasticSearchのURLを指定する必要があるため、ElasticSearchがセットアップされていないと
そもそも動作しません。今回は各ツールごとに完結できる動作確認方法を取りますが、万が一に備えるという意味では
図X.Xのようにデータの連携先→連携元→データ閲覧ツールという流れでセットアップした方が良いでしょう。

#@# インストールの順番図を入れる

** ElasticSearhのインストール
　先にも述べましたが、インストール方法は複数準備されています。導入の目的とご自身の環境に合わせて
ベストなものを選択すると良いでしょう。

1. とにかく使ってみたい場合（Linux）：zipファイル

　「とにかくどんなものか試してみたい！」そんな方はzipファイルを公式サイトからダウンロードしてきましょう。
インストール方法も適当なフォルダにzipファイルを解凍するだけなので導入は簡単に終わります。
ただし、serviceコマンドは付属しないため、長期的な運用を考えている場合には向かないインストール方法です。

2. ちゃんと運用もしたい場合（rpmパッケージを利用するLinux）：rpmパッケージ
　Elastic公式から提供されているrpmパッケージを利用した場合、serviceコマンドが自動的にダウンロードされます。
また、各種設定ファイルやディレクトリ構造はLinuxのディレクトリ形式に合わせて構築されます。
運用を検討している場合は、初めからこちらのやり方を取っておけば環境の再構築を防ぐことができます。

例）
コンフィグファイルの配置先：/etc/elasticsearch/
ログファイルの出力先：/var/log/elasticsearch
基本的なファイルの配置先：/opt/elasticsearch

3. ちゃんと運用もしたい場合（debパッケージを利用するLinux）：debパッケージ
　こちらも2のやり方と同様です。違いはUbuntu系のLinux用パッケージを使うか、
RedHat、OpenSuSE系のLinux用パッケージを使うかだけです。

4. とにかく使ってみたい場合かつ、Docker実行環境がある場合：Dockerコンテナ
　Elastic StackをDocker社が利用していることもあってか、Elastic公式からDockerイメージが提供されています。
手っ取り早く試してみたい場合、かつDockerコンテナの実行環境がある場合は素直にコンテナを利用した方が良いでしょう。
ただし、インストール方式の2.3に移行を考えている場合、構成がかなり変わるので初めからパッケージ利用での環境構築方法を選択した方が良さそうです。

　また、ElasticSearhは物理的にも、Javaのヒープメモリもかなり消費するツールです。
Dockerコンテナ上ではあまり性能が出ないため、大量のデータを流す予定がある場合はコンテナ利用を避けた方が良いです。

5. Puppet、Chef、Ansibleの実行環境がある場合：Githubのレシピを利用
　Elastic公式から各ツールのレシピが共有されています。Githubにアップロードされているため
そちらを用いて環境を構築することも可能です。

6. Windows/Mac上に構築する場合：zipファイル
　Windowsの場合、インストール方法はzipファイル一択となります。Macはbrewコマンドを用いることで
インストールすることもできますが、Elastic公式ではサポートされていないようなので、
今回はインストール方法から対象外としています。

　他の方が検証されているログを見る限り、brewコマンドでインストールした場合はtar.gzパッケージを解凍して
インストールしているように見えるため、公式の最新ファイルをダウンロードしてインストールするのと特に変わらないと思います。
好みで選択すると良いでしょう。

*** zipファイルを用いたインストール

今回は6のzipファイルを用いたインストール方法を取りたいと思います。
zipファイルを用いたインストールの場合、OSが違っていても基本的な手順は同じです。


1. 公式サイトからzipファイルをダウンロード

　Elasticの公式サイトにアクセスし、zipファイルをダウンロードします。

#@# サイトのキャプチャを入れる

2. 作業用ディレクトリを作成（好みで良いです）

　今回は検証用の環境構築なので、アンインストールを簡単にするために作業用ディレクトリを作成します。

#@# ディレクトリ作成用コードを入れる

3. zipファイルの解凍

　ディレクトリ内にダウンロードしたzipファイルを解凍します。

#@# コードを入れる

4. ElasticSearhの起動

　/binディレクトリ下にあるelasticsearchスクリプトを実行し、サービスを起動します。
OSがWindowsの場合、elasticsearch.batを実行します。

#@# コードを入れる

5. 動作確認

　ElasticSearuへクエリを投げ、返り値があるか確認します。

#@# コードを入れる

もしくはGoogle Chrome上でURLにアクセスし、同じような返り値があればOKです。

#@# 画面キャプチャを入れる

返り値が帰ってくれば、正常にインストールできています。

** Logstashのインストール

LogstashもElasticSearhと同様、インストール方式を選択することが可能です。
ただ、ツールごとにインストール方式を分けるやり方は、ディレクトリ構造が異なってしまうため避けた方が良いです。

1. とにかく使ってみたい場合（Linux）：zipファイル

　こちらもElasticSearhと同様、zipファイルを展開するだけでインストールが終了します。
serviceコマンドは付属しません。

2. ちゃんと運用もしたい場合（rpmパッケージを利用するLinux）：rpmパッケージ
　こちらもserviceコマンドの存在や、ディレクトリ構成が自動で割り当てられる点なども
ElasticSearhと同様です。

例）
コンフィグファイルの配置先：/etc/logstash/
ログファイルの出力先：/var/log/logstash
基本的なファイルの配置先：/opt/logstash

3. ちゃんと運用もしたい場合（debパッケージを利用するLinux）：debパッケージ
　こちらもElasticSearhと同様です。

4. とにかく使ってみたい場合かつ、Docker実行環境がある場合：Dockerコンテナ
　ElasticSearhと同様、Elastic社からDockerコンテナが提供されています。
ただし、ElasticSearhとは別のコンテナのため、同時にコンテナを複数起動する必要があります。

　LogstashはRubyで作成されていますが、起動にJavaを必要とします。
こちらもヒープメモリもかなり消費するツールなので注意が必要です。

5. Puppet、Chef、Ansibleの実行環境がある場合：Githubのレシピを利用
　Elastic公式から各ツールのレシピが提供されているので、Githubからクローンして利用することも可能です。

6. Windows/Mac上に構築する場合：zipファイル
　Windowsの場合、インストール方法はzipファイル一択となります。Macはbrewコマンドを用いることで
インストールすることもできますが、Elastic公式ではサポートされていないようなので、
今回はインストール方法から除外しています。

*** zipファイルを用いたインストール

　ElasticSearhのインストール方法に合わせるため、6のzipファイルを用いたインストール方法を取ります。
こちらも、OSごとに手順に差はありません。

1. 公式サイトからzipファイルをダウンロード

　Elasticの公式サイトにアクセスし、zipファイルをダウンロードします。

#@# サイトのキャプチャを入れる

2. zipファイルの解凍

　ElasticSearhをインストールする際に作成したディレクトリに、ダウンロードしたzipファイルを解凍します。
ディレクトリ構成は一例です。

#@# コードを入れる

4. 動作確認用のlogstash.conf作成

　この後の章で詳しく述べますが、Logstashはlogstash.confを読み込むことで
ファイルの取り込み元などを指定します。まずは動作確認用に次のコードをconfファイルへ記述します。

#@# コードを入れる（logstash.confを書くところ全部入れる）

5. 動作確認

　/bin下にあるlogstashスクリプトから、Logstashを起動します。
OSがWindowsの場合、同階層にlogstash.batが配置されているのでそちらを起動しましょう。
logstash.confにタイプミスがない場合、「Logstash startup completed」と出力されます。
コマンドプロンプトに好きな文字列を打ち込んでみましょう。
文字列がそのまま返り値として出力された場合、正しくセットアップできています。

#@# コードを入れる（/bin/logstash -f logstash.conf)

** Kibanaのインストール

　Kibanaも他ツールと同様、インストール方法が複数準備されています。
種類はElasticSearh・Logstashと変わらないため、そちらの章を参照してください。

*** zipファイルを用いたインストール

　ElasticSearhとLogstashのインストール方法に合わせるため、zipファイルを用いたインストール方法を選択しました。
こちらも、OSごとに手順に差はありません。

1. 公式サイトからzipファイルをダウンロード

　Elasticの公式サイトにアクセスし、zipファイルをダウンロードします。
OSの種類によってzipファイルが異なるため、注意が必要です。

#@# サイトのキャプチャを入れる

2. zipファイルの解凍

　ElasticSearhをインストールする際に作成したディレクトリに、ダウンロードしたzipファイルを解凍します。

#@# コードを入れる

3. kibana.ymlの編集

　前にも述べた通り、KibanaはElasticSearhからデータを取得するためElasticSearhのURLを指定する必要があります。
kibana.yml内にURLを指定する箇所があるのでそちらを記述しましょう。

#@# コードを入れる（kibana.ymlの編集）

4. 動作確認

　/binフォルダ下にあるkibanaスクリプトから起動します。（Windowsはkibana.batから起動します。）

#@# コードを入れる（起動）

起動後、ブラウザにhttp://localhost:5601と入力します。図X.Xのような画面が見えていればインストールは完了です。

#@# キャプチャを入れる

* データを集めて可視化しよう（テキストファイル編）

　それでは楽しいElastic Stackライフを！（ラストはこれでしめる）
