= はじめに
　「BIツール」、最近よく聞く言葉になった気がします。巷に溢れる様々なデータを集めて可視化することができる
ため、使われるシーンが増えてきたのでしょう。BIツールを上手く使えばWebサイトへのアクセス分析も
サーバへのアクセス分析も、DBの中の情報さえも整形することができます。
この『ログと情報をレッツ・ラ・まぜまぜ! 〜ELK Stackで作るBI環境〜
』では、BIツールの一種である「Elastic Stack」を取り上げて紹介したいと思います。

== お断り
　本書で扱うElastic Stackのバージョンは全て「5.2」のものとなります。バージョンによってかなり挙動が変わる
コンテンツかつ、バージョンアップが早いコンテンツなのでご注意ください。本書に記載の情報・コマンドはelastic社の公式
ドキュメントを参考にしていますが、導入の際はご自分で動作を確かめてからでお願いいたします。

= BIツールとは
　早速Elastic Stackを触っていこう！と言いたいころですが、
まずはBIツールとはなんぞや？という疑問を解消していきたいと思います。

== BIとは？
　BIとは「business intelligence」の略称です。企業や世の中にある色々な情報を集めて分析し
経営に役立てましょう、ということなんだそうです。これを支援するツールとしてBIツールは存在します。
とはいえ、最近ではサーバ内部の情報（cpuやメモリの使用率・ログイン履歴）などの可視化をすることも
多いようです。

== BIツールの必要性
　「別にBIツールなんてなくてもいいじゃない」と思う方もいらっしゃるかもしれません。でも考えてみてください。
自分のホームページのアクセス履歴を手で調べていくのでしょうか？バズらせたいTwitterのハッシュタグのつぶやき数を
スクロールしながら数えていくのでしょうか？考えただけで気が遠くなりそうです。

　しかし、こんな悩みはBIツールさえあれば解決です。Titterのアクセスログを取り込みさえできれば
ログ収集ツールで数などを分析することができます。ものによっては取り込むログを加工・整形することもできるので
自分で必要な情報をつけて都合が良いデータを作り出すことも可能です。やりすぎると情報工作になってしまいそうですね。

= Elastic Stack
== Elastic Stack（ELK Stack）とは
　Elastic StackはオランダのElastic社が提供しているBIツールです。基本はOSSとして提供されており、誰でも公式サイトから
ダウンロードして使うことが可能です。

== BIツールの中でELK Stackを勧める理由
　数あるBIツールの中からELK Stackをおすすめする理由はいくつかあります。
== Kibanaが使いやすい
　やはり1番の理由はKibanaの使いやすさにあります。直感的に操作することができますし、ぽちぽち画面をいじるだけでそれっぽいグラフを
作成することができます。
== 取り込める情報の種類が多い
　この後に具体的に出てきますが、とにかく取り込みできるログの種類が多いです。テキストファイルはもちろん、
http通信のログ・syslog・windowsのログ・githubのコミット履歴も取得できます。
== ELK Stackだけでログの取り込み・貯める・可視化ができる
　ログの取り込みはfluentd、情報の可視化ツールではGraganaなど、他のツールも存在します。
ただし、「ログの取り込みだけ」「情報の可視化だけ」しかできないものが多く、「情報の取り込み」「情報を貯める」「情報の可視化」がセットでできる
ものは少ないです。Elastic Stackではツールは違えど、全て網羅しています。同じ会社が作っているツールですので、連携のしやすさは1番です。

= ELKを使ってみる
　早速ELK Stackを導入していきましょう。
== 全体像
図を入れる
== Logstash
　Logstashは情報を取り込むためのツールです。取り込む範囲が広いのが特徴で、テキストファイル・syslog・Titterのアクセスログなども
取得することができます。収集するだけではなく、取り込んだものを加工することも可能です。加工する、というのはログにない情報をつけ足す・いらないものは
捨てる、という動作が一例となります。収集した情報はElasticsearchに送るだけではなく、テキストファイルとして出力することも可能です。

== Elasticsearch
　Elasticsearchには収集したデータを保持することができます。Javaで組まれた分散処理型の検索エンジンです。
GithubやFacebookでも使われています。後ほど出てきますが、indexと呼ばれるデータを保持する場所を
分散できるため、データ量によって処理の負荷分散をして性能を上げることも可能です。
バックアップの取得もcurlコマンド一発で取れるのでメンテナンスが楽チンです。

== Kibana
　BIツールの中でも情報を表示する役割を持ちます。Elasticsearchに保持されたデータを可視化し、グラフを作成することができます。
情報内に地理情報があれば、世界地図と組み合わせて情報を表示することができます。
== Beats
　サーバの情報を可視化するツールです。インストールするのみで情報収集をすることができます。
== X-pack（有償ツール）
　Kibanaへのアクセス制御など、かゆいところに手がとどくツールは有償ツールとして提供されています。

= Elasticsearch（情報を貯める）
== Elastcsearchのデータ構造
=== index
　indexはデータの索引的な役割を持っています。Logstashなどで取り込んだデータを検索対象として格納する場所として使われます。
先にも述べましたが、indexは複数持つことができるため、indexを複数作成しデータを分けて管理することもできます。DBでは「スキーマ」に
相当する部分です。
=== type
　取り込んだデータを種類ごとに分けておき、データを検索するときに活用する部分です。DBで言う所の「テーブル」に当たる部分です。
=== field
　DBで言う所の「カラム」に当たる部分です。
=== document
　格納されたドキュメントそのものです。DBでは「レコード」に値する部分です。個々のドキュメントを識別するために、idというデータが
全てに付与されています。（DBと違って主キーを持つことができないため）
=== text
　fieldに入れられたデータです。要素解析（analyze）され分解された単位のデータをtermといいます。
=== mapping
　ドキュメント内のfieldに対する振る舞いを指定します。
例えばname:"MofuMofu"と入れると、"MofuMofu"は文字列なので自動でString型と判断されます。
age:"10"と入れた場合、"10"は数字なのでNumber型と判断されます。

このように、mapppingはfield内のデータに合わせて自動的に付与されますが、都合が悪いときもあります。
例えば"date"というfieldに"20170409"と入っていた場合、自動でNumber型と判断されてしまいます。
"20170409"は数値ではなく、日付として扱いたいときは不便ですよね。なので自分でmappping定義を書き換えることで
"date"というfieldのデータはDate型と定義しなおすことができます。

自分でつけなおすことも可能です。
== Elasticsearchのインストール
=== 事前準備
　ElasticsearchはJava8で動いています。そのためインストールにはOracle JDK 1.8.0_73以上が必要です。
まずはjavaがインストールされているか確認します。
* コード
=== インストール
　javaがインストールされていることが確認できたあと、いよいよElasticsearchをインストールします。
インストール方法はいくつかあります。
* zip/tar.gzパッケージ
　zipファイルを解凍してインストールする方法です。お手軽に環境を立ち上げて検証するような場合は
こちらの方法を利用すると良いでしょう。ただし、serviceコマンドはついてこないため実運用には向きません。
Windowsへのインストールの場合、自動的にzipファイルでのインストール方法を
選択することになります。
* debパッケージ
　Debian系のLinux（Ubuntuなど）へインストールする場合、debパッケージを利用できます。
serviceコマンドが利用できるため、長期運用を考えている場合はこちらを利用してインストールすると良いでしょう。

* rpmパッケージ
　RedHat系のLinux（RHELやCentOSなど）やOpenSUSEなどへインストールする場合、rpmパッケージを利用できます。
rpmパッケージでインストールした場合もserviceコマンドが利用できるため、
長期運用を考えている場合はこちらを利用してインストールすると良いでしょう。

* dockerイメージ
　dockerの実行環境がある場合、コンテナのイメージを利用することもできます。dockerを使い慣れており
検証環境としてElasticsearchが必要な場合はこちらを利用すると良いでしょう。

次の図にインストール方法を選ぶためのフローチャート図を入れたので、参考にしてみてください。
* 図を入れる

また、これ以外にもPuppet・Chef・AnsibleのコードがGithubリポジトリとして提供されているため
これらのツールから環境を準備することも可能です。
Puppet: https://github.com/elastic/puppet-elasticsearch
Chef: https://github.com/elastic/cookbook-elasticsearch
Ansible: https://github.com/elastic/ansible-elasticsearch

== Elasticsearchのセットアップ
== elasticsearch.yml
== Elasticsearchの起動
= Logstash（情報を取り込む）
== Logstashのインストール
== Logstashのセットアップ
== logstash.yml
== logstash.conf
== 動作確認
== Logstashの起動
= Kibana（情報を見せる）
　KibanaはDiscover・Visualize・Dashboard・Settingsの4つから成り立っています。
== Kibanaのインストール
　Kibanaのインストールは他と同様、次の種類があります。先ほどと同様、今回も検証用のため、
tar.gzファイルの解凍をしてインストールしたいと思います。Windowsにインストールする場合、
zipファイルの解凍をしてインストールすることになります。
* tar.gz/zipファイルの解凍
* debパッケージ
* rpmパッケージ
* dockerコンテナのPULL

== Kibanaの起動
== グラフを作ってみる
= 知っておくと便利なもの
= おわりに
　いかがでしたでしょうか。
